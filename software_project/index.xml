<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software_projects | Antonela Tommasel</title>
    <link>https://tommantonela.github.io/software_project/</link>
      <atom:link href="https://tommantonela.github.io/software_project/index.xml" rel="self" type="application/rss+xml" />
    <description>Software_projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 06 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Software_projects</title>
      <link>https://tommantonela.github.io/software_project/</link>
    </image>
    
    <item>
      <title>graphoW</title>
      <link>https://tommantonela.github.io/software_project/graphow/</link>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://tommantonela.github.io/software_project/graphow/</guid>
      <description>&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;graphoW is a Python package for the creation of a Graph-of-Words (GoW) representation of texts.&lt;/p&gt;
&lt;p&gt;Structure is based on: &amp;ldquo;Graph-of-word and TW-IDF: new approach to ad hoc IR&amp;rdquo;
Graph metrics are based on: &amp;ldquo;Graph analysis of dream reports is especially informative about psychosis&amp;rdquo;
Narrative consistency and rapidity are based on: &amp;ldquo;Measuring Narrative Fluency by Analyzing Dynamic Interaction Networks in Textual Narratives&amp;rdquo;&lt;/p&gt;
&lt;p&gt;It allows to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a GoW for individual texts.&lt;/li&gt;
&lt;li&gt;Create a container of GoW in which each GoW corresponds to a paragraph in the text.&lt;/li&gt;
&lt;li&gt;Compute diverse types of graph metrics (e.g., individual, global, connectivity&amp;hellip;).&lt;/li&gt;
&lt;li&gt;Compute the narrative consistency of text based on all terms or only on noun phrases.&lt;/li&gt;
&lt;li&gt;Compute the rapidity of a text, i.e., how slow/fast change the structure of paragraphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;graphoW is licenced under the Apache License V2.0. Copyright 2021 - ISISTAN - UNICEN - CONICET&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Faking It!</title>
      <link>https://tommantonela.github.io/software_project/fakingit/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://tommantonela.github.io/software_project/fakingit/</guid>
      <description>&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;Social media has become the primary source of news for their users. Besides fostering social connections between persons, social networks also represent the ideal environment for undesirable phenomena, such as the dissemination of unwanted or aggressive content, misinformation and fake news, which all affect the individuals as well as the society as a whole. Thereby, in the last few years, the research on misinformation has received increasing attention. Nonetheless, even though some computational solutions have been presented, the lack of a common ground and public datasets has become one of the major barriers. Not only datasets are rare, but also, they are mostly limited to only the actual shared text, neglecting the importance of other features, such as social content and temporal information. In this scenario, this project proposes the creation of a publicly available dataset, comprising multi-sourced data and including diverse features related not only to the textual and multimedia content, but also to the social context of news and their temporal information. This dataset would not only allow tackling the task of fake news detection, but also studying their evolution, which, in turn, can foster the development of mitigation and debunking techniques.&lt;/p&gt;
&lt;p&gt;The Figure shows the diffent types of data that can be retrived using Faking It!.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/fakingit_pipeline.png&#34; alt=&#34;Faking It! Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;In summary, Faking It! allows to collect:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Web content retrieved from urls given as input.&lt;/li&gt;
&lt;li&gt;Social posts referring to the selected news.&lt;/li&gt;
&lt;li&gt;Temporal diffusion of posts trajectory (for example, retweet or replies chains).&lt;/li&gt;
&lt;li&gt;URL and images accompanying the retrieved posts.&lt;/li&gt;
&lt;li&gt;For each user engaged in the diffusion process, the profile, posts and social network.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The original aim of the tool was collecting datasets related to the evolution and propagation of fake news. In this context, the collected multi-source datasets have the potential to contribute in the study of various open research problems related to fake news. The augmented set of features provides a unique opportunity to play with different techniques for detecting and analysing the evolution of fake news. Moreover, the temporal information enables the study of the processes of fake news diffusion and evolution, and the roles that users play in such processes through social media reactions. The proposed dataset could be the starting point of diverse exploratory studies and potential applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Fake news detection&lt;/em&gt;. One of the main difficulties is the lack of reliable labelled data and an extended feature set. The presented dataset aims at providing reliable labels annotated by humans and multi-sourced features including text, images, social context and temporal information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Fake news evolution and engagement cycle&lt;/em&gt;. The diffusion process involves different stages in usersâ€™ engagement and reaction. Using the temporal information could help to study how stories become viral and their diffusion trajectories across social media. This would also allow to study the roles that users play on the diffusion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Fake and malicious account detection&lt;/em&gt;. These types of accounts include social bots, trolls and spammers, which can all boost the spread of fake news. The user features in the dataset would allow studying the characteristics of such type of users, how they differentiate from legitimate users and how they contribute to the spread of disinformation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Fake news debunking&lt;/em&gt;. Knowing the evolution and trajectories of fake news could help in the study of the debunking process. Likewise, user reactions and answers in social media can provide useful elements to help to determine the veracity of a piece of news.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ASPredictor</title>
      <link>https://tommantonela.github.io/software_project/as-predictor/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://tommantonela.github.io/software_project/as-predictor/</guid>
      <description>&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;This prototype tool supports the prediction of dependency-based Architectural Smells and also the evaluation of those predictions using a sequence of system versions. ASPredictor is designed as a pipeline of processing components. Each component receives a number of inputs and generates outputs for other components downstream.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/aspredictor_pipeline.jpg&#34; alt=&#34;AsPredictor Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;The main functionality consists of three steps, namely: pre-processing, classification construction, and smell prediction.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Extract-Detect.&lt;/em&gt; Parses Java source code from predefined versions and extracts packages dependency graphs, relying on a Java static analyzer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Dep-Pred.&lt;/em&gt; Takes two dependency graphs as inputs, and produces an ordered list of predicted dependencies, relying on a trained binary classifier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Smell-Pred.&lt;/em&gt; Receives dependency graphs and the predicted dependencies, and identifies potentially ”new” AS in the expanded graph to appear in the next system version, which are ranked according to their relevance. Currently, three types of smells are supported: cyclic dependencies, hub-like dependencies and unstable dependencies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Eval.&lt;/em&gt; Compares the predicted dependencies and/or smells with the actual dependencies/smells, and computes typical Machine Learning metrics such as: precision, recall, F-measure and nDCG.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ASPredictor is licenced under the Apache License V2.0. Copyright 2019 - ISISTAN - UNICEN - CONICET&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Sen4Smells</title>
      <link>https://tommantonela.github.io/software_project/sens4smells/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://tommantonela.github.io/software_project/sens4smells/</guid>
      <description>&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;Sen4Smells is tool for prioritizing architecture-sensitive smells based on a technical debt index (e.g., ADI, SDI, etc.) Sen4Smells is licenced under the Apache License V2.0.&lt;/p&gt;
&lt;p&gt;The main functionality of the Sen4Smells is providing assistence to engineers for interpretating Technical Debt metrics in terms problematic Architectural Smells and system packages.&lt;/p&gt;
&lt;p&gt;Sen4Smells is able to perform a sensitivity analysis for a collection of system values provided by a predetermined debt index. Our approach relies on two building blocks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The adaptation of an existing SA method to AS-based debt indices.&lt;/li&gt;
&lt;li&gt;A decomposition strategy for dealing with the index at different granularity levels.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By leveraging on Architectural Smells, the goal of the Sensitivity Analysis is to understand how variations in the Technical Debt can be attributed to variations in features of system elements. To do so, the sensitivity analysis performs a screening of the multiple variables affecting the index over time, and returns the most sensitive ones (i.e., key elements) to the engineer (tool user). The inputs for this analysis are: a set of previous system versions, the formula for computing a particular Technical Debt, and the granularity level for the variables (e.g., smell types, individual smells, or packages).&lt;/p&gt;
&lt;p&gt;The tool is designed as pipeline, in which existing modules for detecting smells and computing metrics from the source code can be configured. These smells and metrics depend on the Technical Debt under consideration, which is also a parameter for the pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../img/sen4smells_pipeline.jpg&#34; alt=&#34;Sen4Smells Pipeline&#34;&gt;&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>SMArtOp</title>
      <link>https://tommantonela.github.io/software_project/smartop/</link>
      <pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://tommantonela.github.io/software_project/smartop/</guid>
      <description>&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;SMArtOp is a Java library for dividing the processing of large-scale sparse-matrix arithmetic operations on distributed environments.The software is designed for dividing and balancing the processing of large-scale sparse-matrix arithmetic operations into simpler and independent tasks to be executed in a distributed environment.&lt;/p&gt;
&lt;p&gt;SMArtOp is licenced under the Apache License V2.0. Copyright 2016 - ISISTAN - UNICEN - CONICET&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
